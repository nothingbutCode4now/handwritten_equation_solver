{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization of CNN Model"
      ],
      "metadata": {
        "id": "yFjgOHptiVDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do upload the model folder from github before running this cell"
      ],
      "metadata": {
        "id": "APmNmdGnjM9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from os.path import isfile, join\n",
        "from keras import backend as K\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def extract_imgs(img):\n",
        "    img = ~img # Invert the bits of image 255 -> 0\n",
        "    _, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) # Set bits > 127 to 1 and <= 127 to 0\n",
        "    ctrs, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnt = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0]) # Sort by x\n",
        "\n",
        "    img_data = []\n",
        "    rects = []\n",
        "    for c in cnt :\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        rect = [x, y, w, h]\n",
        "        rects.append(rect)\n",
        "\n",
        "    bool_rect = []\n",
        "    # Check when two rectangles collide\n",
        "    for r in rects:\n",
        "        l = []\n",
        "        for rec in rects:\n",
        "            flag = 0\n",
        "            if rec != r:\n",
        "                if r[0] < (rec[0] + rec[2] + 10) and rec[0] < (r[0] + r[2] + 10) and r[1] < (rec[1] + rec[3] + 10) and rec[1] < (r[1] + r[3] + 10):\n",
        "                    flag = 1\n",
        "                l.append(flag)\n",
        "            else:\n",
        "                l.append(0)\n",
        "        bool_rect.append(l)\n",
        "\n",
        "    dump_rect = []\n",
        "    # Discard the small collide rectangle\n",
        "    for i in range(0, len(cnt)):\n",
        "        for j in range(0, len(cnt)):\n",
        "            if bool_rect[i][j] == 1:\n",
        "                area1 = rects[i][2] * rects[i][3]\n",
        "                area2 = rects[j][2] * rects[j][3]\n",
        "                if(area1 == min(area1,area2)):\n",
        "                    dump_rect.append(rects[i])\n",
        "\n",
        "    # Get the final rectangles\n",
        "    final_rect = [i for i in rects if i not in dump_rect]\n",
        "    for r in final_rect:\n",
        "        x = r[0]\n",
        "        y = r[1]\n",
        "        w = r[2]\n",
        "        h = r[3]\n",
        "\n",
        "        im_crop = thresh[y:y+h+10, x:x+w+10] # Crop the image as most as possible\n",
        "        im_resize = cv2.resize(im_crop, (28, 28)) # Resize to (28, 28)\n",
        "        im_resize = np.reshape(im_resize, (1, 28, 28)) # Flat the matrix\n",
        "        img_data.append(im_resize)\n",
        "\n",
        "    return img_data\n",
        "\n",
        "class ConvolutionalNeuralNetwork:\n",
        "    def __init__(self):\n",
        "        if os.path.exists('/content/model.json') and os.path.exists('/content/model_weights.h5'):\n",
        "            self.load_model()\n",
        "        else:\n",
        "            self.create_model()\n",
        "            self.train_model()\n",
        "            self.export_model()\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        print('Loading Model...')\n",
        "        model_json = open('/content/model.json', 'r')\n",
        "        loaded_model_json = model_json.read()\n",
        "        model_json.close()\n",
        "        loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "        print('Loading weights...')\n",
        "        loaded_model.load_weights(\"/content/model_weights.h5\")\n",
        "\n",
        "        self.model = loaded_model\n",
        "\n",
        "    def predict(self, operationBytes):\n",
        "        Image.open(operationBytes).save('_aux_.png')\n",
        "        img = cv2.imread('_aux_.png',0)\n",
        "        os.remove('_aux_.png')\n",
        "        if img is not None:\n",
        "            img_data = extract_imgs(img)\n",
        "\n",
        "            operation = ''\n",
        "            for i in range(len(img_data)):\n",
        "                img_data[i] = np.array(img_data[i])\n",
        "                img_data[i] = img_data[i].reshape(-1, 28, 28, 1)\n",
        "\n",
        "                pred = self.model.predict(img_data[i])\n",
        "                result=np.argmax(pred,axis=1)\n",
        "                print(result[0])\n",
        "                if result[0] == 10:\n",
        "                    operation += '+'\n",
        "                elif result[0] == 11:\n",
        "                    operation += '-'\n",
        "                elif result[0] == 12:\n",
        "                    operation += 'x'\n",
        "                elif result[0] == 13:\n",
        "                    operation += 'a'\n",
        "                elif result[0] == 14:\n",
        "                    operation += 'b'\n",
        "                else:\n",
        "                    operation += str(result[0])\n",
        "            print(f\"Operation is {operation}\")\n",
        "            return operation\n",
        "CNN = ConvolutionalNeuralNetwork()\n"
      ],
      "metadata": {
        "id": "7P6c6E7zikDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0ee728-ced1-40b8-d3eb-946564ee981c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model...\n",
            "Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Polynomial Solving Code"
      ],
      "metadata": {
        "id": "HQWqlZWo4yFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary Modules\n",
        "from google.colab.patches import cv2_imshow\n",
        "import base64\n",
        "from sympy.parsing.sympy_parser import parse_expr\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "from sympy import symbols, Eq, solve\n",
        "im = cv2.imread(\"/content/eq7.jpeg\",0) # Read the User Image\n",
        "im = cv2.resize(im,(600,200)) # Resize the User Image as per Model Requirement\n",
        "\n",
        "# Thresholding the User Image\n",
        "ret, thresh = cv2.threshold(im, 127, 255, 0)\n",
        "kernel = np.zeros((3,3),np.uint8)\n",
        "thresh = cv2.dilate(thresh,kernel,iterations = 1)\n",
        "thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "thresh = thresh[10:,10:]\n",
        "cv2_imshow(thresh)\n",
        "\n",
        "\n",
        "\n",
        "# Conversion of the threshold image into base64 DataURL\n",
        "# This is done as our model requires base64 string to detect Equation/Polynomial\n",
        "\n",
        "retval, buffer_img= cv2.imencode('.png', thresh)\n",
        "data = base64.b64encode(buffer_img)\n",
        "data = str(data)\n",
        "data = str(data)[2:len(data)-1]\n",
        "operation = BytesIO(base64.urlsafe_b64decode(data))\n",
        "\n",
        "#Initializing the CNN model\n",
        "def modifier1(s):\n",
        "  x = re.split(r'\\+|\\-',s)\n",
        "  order = ''\n",
        "  for i in s:\n",
        "    if i=='+':\n",
        "        order+='+'\n",
        "    elif i == '-':\n",
        "        order+='-'\n",
        "  first = x[0]\n",
        "  second = x[1]\n",
        "  x[0] = first[:-1]+'*x**2'\n",
        "  x[1] = second[:-1]+'*x' if len(second)>1 else second+'*x'\n",
        "  eqn = x[0]+order[0]+x[1]+order[1]+x[2]\n",
        "  return eqn\n",
        "\n",
        "CNN = ConvolutionalNeuralNetwork()\n",
        "operation = CNN.predict(operation) # Feeding the base64 string to the the CNN Model\n",
        "eqn = modifier1(operation)\n",
        "x = symbols('x')\n",
        "eqn = parse_expr(eqn)\n",
        "\n",
        "print(eqn)\n",
        "solve((eqn), (x))"
      ],
      "metadata": {
        "id": "mQUpKJ2C48M5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "e617fe2f-d125-4d5b-8d42-ae4413d0ea73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=590x190 at 0x7F73082F23E0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model...\n",
            "Loading weights...\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "7\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "6\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "2\n",
            "Operation is 72+6+2\n",
            "7*x**2 + 6*x + 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-3/7 - sqrt(5)*I/7, -3/7 + sqrt(5)*I/7]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Equation in Two Variable"
      ],
      "metadata": {
        "id": "r8NAcGTX7qH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary Modules\n",
        "import re\n",
        "import numpy as np\n",
        "from sympy.parsing.sympy_parser import parse_expr\n",
        "from sympy import symbols, Eq, solve\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "ls = []\n",
        "filenames = [\"/content/eq21.jpeg\",\"/content/eq22.jpeg\"] #filenames for the linear equations\n",
        "#Run for loops for two times to get the two eqyations from the images specifed as filenames\n",
        "for i in range(0,2):\n",
        "  im = cv2.imread(filenames[i],0)# Read the User Image\n",
        "  im = cv2.resize(im,(600,200))# Resize the User Image as per Model Requirement\n",
        "\n",
        "  # Thresholding the User Image\n",
        "\n",
        "  ret, thresh = cv2.threshold(im, 127, 255, 0)\n",
        "  kernel = np.zeros((3,3),np.uint8)\n",
        "  thresh = cv2.dilate(thresh,kernel,iterations = 1)\n",
        "  thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "  thresh = thresh[10:,10:]\n",
        "  cv2_imshow(thresh)\n",
        "\n",
        "\n",
        "  # Conversion of the threshold image into base64 DataURL\n",
        "  # This is done as our model requires base64 string to detect Equation/Polynomial\n",
        "\n",
        "  retval, buffer_img= cv2.imencode('.png', thresh)\n",
        "  data = base64.b64encode(buffer_img)\n",
        "  data = str(data)\n",
        "  data = str(data)[2:len(data)-1]\n",
        "\n",
        "  #Initializing the CNN model\n",
        "  operation = BytesIO(base64.urlsafe_b64decode(data))\n",
        "  CNN = ConvolutionalNeuralNetwork()\n",
        "  operation = CNN.predict(operation)\n",
        "  ls.append(operation)\n",
        "\n",
        "\n",
        "def modifier(s):\n",
        "    x = re.split(r'\\+|\\-',s)\n",
        "\n",
        "    order = ''\n",
        "    eqn = ''\n",
        "\n",
        "    for i in s:\n",
        "        if i=='+':\n",
        "            order+='+'\n",
        "        elif i == '-':\n",
        "            order+='-'\n",
        "\n",
        "    for i in range(len(x)-1):\n",
        "        elem = x[i]\n",
        "        if len(elem)>1:\n",
        "          x[i] = elem[:-1]+'*'+elem[-1]\n",
        "\n",
        "    eqn = x[0]+order[0]+x[1]+order[1]+x[2]\n",
        "    return eqn\n",
        "\n",
        "#putting both equation in variables\n",
        "eq1 = modifier(ls[0])\n",
        "eq2 = modifier(ls[1])\n",
        "\n",
        "#Initializing the solving process using the sympy module\n",
        "\n",
        "x, y = symbols('a b')\n",
        "eq1 = parse_expr(eq1)\n",
        "eq1 = Eq((eq1))\n",
        "eq2 = parse_expr(eq2)\n",
        "eq2 = Eq((eq2))\n",
        "\n",
        "print(eq1)\n",
        "\n",
        "print(eq2)\n",
        "\n",
        "\n",
        "# solving the equation\n",
        "print(\"Values of 2 unknown variable are as follows:\")\n",
        "\n",
        "print(solve((eq1, eq2), (x, y)))\n"
      ],
      "metadata": {
        "id": "CIKotEvr7tMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80db77ec-f996-4813-9e1a-c7df92ee8241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=590x190 at 0x7F72F8E644F0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAC+CAAAAADmP98ZAAAGbklEQVR4nO2d2XbjKhBFpbv6/3+Z+xAn1oREiQNVSHu/9ErHsRm2ihnPaQJQ8Z93AuBJoBMIQScQgk4gBJ1ACDqBEHQCIegEQtAJhKATCEEnEIJOIASdQAg6gRB0AiHoBELQCYSgEwhBJxCCTiAEnUAIOoEQdAIh6ARC0AmEoBMIQScQgk4g5J93AoZgniauBimB6ARC0KmA2TsBw4BOhaBUCegEQtAJhKBTKbR2BaATCEEnEKKZxvxrCB452feTu0dmTY1Ap2WnYqbQX029TusuKj71ZztIcKyCWp124x186sHpMPP3lw41UdkVZ/TsQlmxO1ROnU5HCcawF1Oj04w5selfP6L9Tp9mep4mek9vpiI6fSeb0q9AiNSHsOV8X6e/8cMyb2HzCV2oXmQ5EIgulQspwLNc2XcKkIMXsy19/9qo0sk/+RCLx55k4fCJBzV9p8jVRf/Nhfs6Rej5nYNS3WH73LgEfJ7RqZCAdRcQsU60L+/m0dFJKTcPSglanSjzl/PQ6ITXPkinMT+VWN5rddyGCi1Q6mQNCd/Xj7JFapHDQVLcGaFORptWLx/Ap3n7Y/wk90fXdzIe3dzVTlhySXPf2+z9+QfIdKqJTaPyiExIETV21k7FviLm4r8NxABtdF/EdxSYX59WPw5XPcMluDGSxm5pk6l8P7sSvpsThms+hktwW8RXXtj+4qteClwt6aAlD5xcV+qjk32INuzk5TeKfs+CIdYS+ZUX5d2J1euahCetr5t3+02xrftUnM3zd40azet0OszU1QRf0JLIkls6slbpaPm+Q5OjB0UPbNOmLm2Ot6+qUvXJH5/KwpN58FvwrvHGlVU6ZYvoLJ/mdeKKtDQOCIb41CYlu3f19qvunN0tn8TcGFjKEufWhcku/Ey+SrXa75QX7eefTJ7tRfGGHokNz7VE7YU8/Se4nW1K8xSwC+OXoDqd0m6lLZVV8C6/N7XoZ1OLGlq/5/wpvTRdC3HRyrrtntFfeXE+H/Owtql8Gul67jYtfl1tg1OAajBRUNA/9ewL3/zs2vpJ2ksTdis/24Lw8anFvJNpPqYHIRLSuLU8dKo7o9+gko5Hio0mt7Y4Vt9hXE7em33afGi2NnNfbyJfFRZ+j0pezU7SZsj2t8e9pj5DiNalOd6TFLlSdiz9psc2zeUtDk5K8hE1HstrufvSSKdTLzo8PR0+wnhypytuSep7qPxsqWks3E9FnfPjU/80ttIp4CMrZN7O+oTDKVUhrryI/aT/sEhj8MjkiYNO13uhQrORKWZw8prW7LpBJZO7EST6TeU2MkW1yYnRZ8X7Em73YzQa6WQJOGMEp6N0ItOWEF3xQcGmHW2i0xOD0wZcOqJJdLq/BjmKW/G/8sGHFtHJ5MQoAi0ZwSWfcm0QnW5lJPCR/923xo1jU/eU6nXa3N1U9up0tGIfRa108lNQvIpO3tjdsmn1P9/lyyjbg9PkvVXOiNuDqNbJlpFln/1vt2qUoLRmfwYsLn4lKNbJttIeU5w8Y8jkuRNLq9NNmz5XGsa2axCXfJ/RVt968MDYNATGOlAjHNld52S1R3DfZ99fyCVJ14twHwxbotPpOVbDhVzz+iaDMS5cHYAAe2dUjZ3terecNsn2NrAggExtrqk/udXhwKP2N66+gCjFZtTpcGKx6rnY3oc77jXRHcgc/L16SPtRHZ0qNyjuX60piKeGuZ1Qh/kc5rqwTXasOxSfWssd6XjdkR17dJrX4/01lznp2tl+ZXvpmmnLKuvlc1H0Xt1OPEZZQBYSOjJNk7HML3JT/FZOm3EewJVP3mWqW2Qx5MQ70w8lQLGqvqAmQFbeQbYKQtSAsYORyUyIrLyE4yoIUgPm/iqHF93ZVkGgCrgx/GFdLQLae6VV3BtNMzSDQx44OQN+cEcBCEEnEIJOIASdQAg6gRB0AiHoBELQCYSgEwhBJxCCTiAEnUAIOoEQdAIh6ARC0AmEoBMIQScQgk4gBJ1ACDqBEHQCIegEQtAJhKATCEEnEIJOIASdQAg6gRB0AiHoBELQCYSgEwhBJxCCTiAEnUAIOoEQdAIh6ARC0AmEoBMIQScQgk4gBJ1ACDqBEHQCIegEQtAJhKATCEEnEIJOIASdQAg6gRB0AiHoBELQCYSgEwhBJxCCTiAEnUAIOoEQdAIh6ARC0AmEoBMIQScQgk4gBJ1ACDqBkP8BanUS5CQDRP8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model...\n",
            "Loading weights...\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "5\n",
            "Operation is 3a+b-5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=590x190 at 0x7F72FBCD6AD0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model...\n",
            "Loading weights...\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "13\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "10\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "14\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "11\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "6\n",
            "Operation is a+3b-6\n",
            "Eq(3*a + b - 5, 0)\n",
            "Eq(a + 3*b - 6, 0)\n",
            "Values of 2 unknown variable are as follows:\n",
            "{a: 9/8, b: 13/8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-647f0b3e5363>:71: SymPyDeprecationWarning: \n",
            "\n",
            "Eq(expr) with a single argument with the right-hand side\n",
            "defaulting to 0 is deprecated. Use Eq(expr, 0) instead.\n",
            "\n",
            "See https://docs.sympy.org/latest/explanation/active-deprecations.html#deprecated-eq-expr\n",
            "for details.\n",
            "\n",
            "This has been deprecated since SymPy version 1.5. It\n",
            "will be removed in a future version of SymPy.\n",
            "\n",
            "  eq1 = Eq((eq1))\n"
          ]
        }
      ]
    }
  ]
}